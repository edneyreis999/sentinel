# Relat√≥rio de Entrevista: Stack RAG Alta Fidelidade para GDDs
**Projeto:** Sentinel
**Data:** 2026-01-29
**Contexto Original:** [Stack RAG Alta Fidelidade para GDDs.md](../pesquisas/Stack%20RAG%20Alta%20Fidelidade%20para%20GDDs.md)
**Objetivo:** Implementa√ß√£o T√©cnica

---

## Sum√°rio Executivo

Este documento consolida as decis√µes arquiteturais tomadas durante entrevista estruturada para implementa√ß√£o de um sistema de **Retrieval-Augmented Generation (RAG) de alta fidelidade** focado em Documentos de Game Design (GDD) narrativos no projeto Sentinel.

A entrevista esclareceu **19 gaps cr√≠ticos** identificados no documento de pesquisa original, transformando conhecimento estado-da-arte em decis√µes execut√°veis, priorizando **simplicidade operacional, baixo custo inicial e valida√ß√£o r√°pida de valor** para um MVP.

### ‚ö° Decis√£o Chave: Claude 3.5 Sonnet via Plano Pro

**Contexto:** O desenvolvedor possui plano Claude Pro ($20/m√™s flat fee) com acesso ao Claude CLI local. A decis√£o foi usar **Claude 3.5 Sonnet** (via Anthropic API) em vez de GPT-4o, aproveitando a mesma API key do plano Pro.

**Benef√≠cios:**
- ‚úÖ **Custo zero adicional no MVP** (conta no limite do plano Pro: ~150-200 mensagens/dia)
- ‚úÖ **Qualidade superior para narrativa:** Claude √© melhor em coer√™ncia narrativa e worldbuilding
- ‚úÖ **Menos alucina√ß√µes em RAG:** Taxa inferior de alucina√ß√µes comparado a GPT-4o em tarefas de retrieval
- ‚úÖ **Contexto maior:** 200k tokens vs 128k do GPT-4o
- ‚úÖ **Uso em produ√ß√£o futura:** Continuar√° usando Claude (plano Pro ou pay-per-use conforme escala)

**Nota T√©cnica:** O Claude CLI usa a Anthropic API por baixo dos panos. A mesma API key funciona tanto no CLI quanto no c√≥digo NestJS via SDK `@anthropic-ai/sdk`, garantindo portabilidade total (desenvolvimento local, Docker, VPS).

---

## üéØ Abordagem Arquitetural Escolhida

### **Arquitetura Monol√≠tica Simplificada no NestJS**

**Decis√£o:** Implementar o sistema RAG como um m√≥dulo isolado dentro do monolito NestJS existente (Sentinel), com PostgreSQL como banco de dados √∫nico (grafos + vetores + dados relacionais) e APIs externas para LLM/embeddings.

#### **Justificativa:**

1. **Valida√ß√£o antes de Complexidade:** O documento de pesquisa descreve o estado-da-arte de produ√ß√£o (2026), mas **n√£o √© um ponto de partida**. A arquitetura monol√≠tica permite validar a proposta de valor ("GraphRAG melhora a qualidade das respostas sobre GDD?") antes de investir em microservi√ßos, orquestra√ß√£o e infraestrutura complexa.

2. **Simplicidade Operacional:** Um √∫nico reposit√≥rio, uma linguagem principal (TypeScript), um banco de dados, deploy √∫nico. Zero overhead de comunica√ß√£o inter-servi√ßos (gRPC/NATS se torna desnecess√°rio).

3. **Custo M√≠nimo:**
   - Postgres self-hosted (Docker Compose)
   - HuggingFace Inference API (free tier para embeddings)
   - Claude 3.5 Sonnet via plano Pro existente ($20/m√™s flat fee, sem custo adicional no MVP)
   - VPS tradicional ($10-40/m√™s vs $200+ de managed services)

4. **Caminho de Evolu√ß√£o Claro:** A arquitetura permite migra√ß√£o futura para microservi√ßos se/quando necess√°rio:
   - **Trigger:** Lat√™ncia de embeddings via API > 500ms P95, ou necessidade de modelos locais (ColBERT, cross-encoders) por custo/privacidade
   - **Migra√ß√£o:** Extrair m√≥dulo `gdd-rag` para servi√ßo Python/FastAPI separado, adicionar gRPC/NATS

**Refer√™ncia ao Contexto Original:**
> *"A stack tecnol√≥gica para sustentar este pipeline complexo em 2026 baseia-se em uma arquitetura de microservi√ßos orquestrada por NestJS, atuando como o Backend-for-Frontend (BFF)."* (linhas 48-51 do documento pesquisado)

**Esclarecimento:** Essa √© a arquitetura **final** de produ√ß√£o descrita no documento. Para MVP, come√ßamos com monolito e evolu√≠mos conforme necess√°rio (princ√≠pio YAGNI - You Aren't Gonna Need It).

---

## üìä Decis√µes T√©cnicas Consolidadas

### 1. Arquitetura de Dados

#### **1.1 Banco de Grafos: Apache AGE (PostgreSQL Extension)**

**Gap Esclarecido:** *"O documento menciona grafos de conhecimento mas n√£o especifica qual banco usar entre Neo4j, Memgraph, FalkorDB, JanusGraph, etc."*

**Decis√£o:** PostgreSQL + Apache AGE

**Justificativa:**
- **Infraestrutura Unificada:** Grafos + vetores + dados relacionais no mesmo DB (zero custo adicional de licenciamento)
- **Simplicidade:** Elimina complexidade de gerenciar m√∫ltiplos bancos
- **Cypher Compat√≠vel:** AGE suporta Cypher query language (padr√£o da ind√∫stria para grafos)
- **Performance Suficiente:** Para escala m√©dia (centenas de n√≥s/milhares de arestas no MVP), AGE √© adequado

**Trade-off Aceito:** Performance inferior ao Neo4j/Memgraph em grafos massivos (milh√µes de n√≥s), mas irrelevante para MVP.

**Refer√™ncia ao Contexto Original:**
> *"A base de um sistema de RAG de alta fidelidade para 2026 come√ßa na fase de ingest√£o. O paradigma de 'Garbage In, Garbage Out' √© intensificado em documentos complexos como GDDs, onde a fragmenta√ß√£o arbitr√°ria de texto (chunking) destr√≥i a continuidade contextual necess√°ria para entender mec√¢nicas de RPG. A solu√ß√£o adotada pela ind√∫stria envolve a constru√ß√£o de grafos de conhecimento orientados por ontologias."* (linhas 19-23)

---

#### **1.2 Vector Database: pgvector (PostgreSQL Extension)**

**Gap Esclarecido:** *"O documento fala em 'busca vetorial' mas n√£o especifica entre Pinecone, Qdrant, Weaviate, pgvector, Redis Vector, etc."*

**Decis√£o:** pgvector no PostgreSQL

**Justificativa:**
- **Coes√£o Arquitetural:** Mant√©m tudo no Postgres (AGE + pgvector + dados relacionais)
- **Zero Custo Extra:** Extens√£o open-source, sem licen√ßas adicionais
- **Performance Adequada:** HNSW index suporta escala m√©dia (10k-100k vetores) com lat√™ncia <100ms
- **Queries Cross-Domain:** Pode combinar busca vetorial + queries SQL relacionais em uma √∫nica transa√ß√£o

**Migra√ß√£o Futura:** Se ultrapassar 500k vetores ou precisar de filtros avan√ßados, migrar para Qdrant (self-hosted) ou Weaviate mantendo a mesma l√≥gica de aplica√ß√£o.

**Refer√™ncia ao Contexto Original:**
> *"A precis√£o sem√¢ntica em 2026 √© alcan√ßada atrav√©s de um pipeline de recupera√ß√£o em m√∫ltiplos est√°gios. O primeiro est√°gio foca em recall (abrang√™ncia), utilizando buscas h√≠bridas que combinam vetores densos (para significado sem√¢ntico) e vetores esparsos como BM25 (para precis√£o de palavras-chave t√©cnicas e nomes pr√≥prios)."* (linhas 32-34)

---

#### **1.3 Ontologia de Dom√≠nio: Narrativa RPG**

**Gap Esclarecido:** *"O documento diz 'construir ontologia de dom√≠nio RPG' mas n√£o define quais entidades, rela√ß√µes, formato (OWL/RDF/custom), ou como modelar versionamento."*

**Decis√£o:** Ontologia Narrativa com Relacionamentos Emocionais/Tem√°ticos

**Entidades Core:**
- **Personagem** (NPCs, protagonistas)
- **Fac√ß√£o/Organiza√ß√£o** (reinos, guildas)
- **Localiza√ß√£o** (cidades, dungeons, regi√µes)
- **Evento** (marcos hist√≥ricos)
- **Lore/Conceito** (mitologia, cosmologia)
- **Miss√£o/Quest** (arcos narrativos)
- **Relacionamento** (amizade, rivalidade, amor, trai√ß√£o)
- **Arco de Personagem** (desenvolvimento emocional)
- **Tema Narrativo** (temas explorados na hist√≥ria)

**Rela√ß√µes (Arestas do Grafo):**

**Estruturais:**
- `PERTENCE_A` (Personagem ‚Üí Fac√ß√£o)
- `LOCALIZADO_EM` (Personagem/Evento ‚Üí Localiza√ß√£o)
- `PARTICIPA_DE` (Personagem ‚Üí Miss√£o/Evento)
- `RELACIONADO_COM` (Lore ‚Üí Lore)
- `MENCIONA` (Miss√£o ‚Üí Personagem/Localiza√ß√£o/Lore)

**Temporais:**
- `ACONTECE_ANTES/DEPOIS` (Evento ‚Üí Evento)
- `DESENCADEIA` (Evento ‚Üí Miss√£o)
- `CONTRADIZ` (Lore ‚Üí Lore)
- `VERS√ÉO_DE` (para versionamento de GDD)

**Emocionais/Tem√°ticas:**
- `TEM_RELACIONAMENTO` (Personagem ‚Üí Relacionamento ‚Üí Personagem)
- `EVOLUI_EM` (Personagem ‚Üí Arco)
- `EXPLORA_TEMA` (Miss√£o/Arco ‚Üí Tema)
- `CONFLITA_COM` (Fac√ß√£o ‚Üí Fac√ß√£o)
- `MOTIVA` (Evento ‚Üí Personagem)

**Justificativa:**
- Foco em **worldbuilding e narrativa** (n√£o mec√¢nicas de combate/progress√£o inicialmente)
- Suporta queries complexas: *"Quais personagens da Fac√ß√£o X t√™m rivalidade com NPCs da Localiza√ß√£o Y durante o Evento Z?"*
- Permite an√°lise tem√°tica e rastreamento de arcos emocionais (character-driven stories)
- **Expans√£o Futura:** Adicionar entidades mec√¢nicas (Classe, Habilidade, Atributo, Item) ap√≥s validar o sistema narrativo

**Refer√™ncia ao Contexto Original:**
> *"Ao alinhar o grafo de conhecimento com uma ontologia extra√≠da de bancos de dados relacionais est√°veis do est√∫dio, reduz-se drasticamente o custo computacional de infer√™ncias repetidas de LLM e elimina-se a necessidade de pipelines complexos de fus√£o de ontologias."* (linhas 22-24)

**Esclarecimento:** N√£o temos "bancos de dados relacionais est√°veis do est√∫dio" no MVP. A ontologia ser√° definida manualmente com base no GDD real do Sentinel e refinada iterativamente.

---

### 2. Pipeline de Retrieval & Reranking

#### **2.1 Estrat√©gia de Chunking: Sem√¢ntico por Se√ß√£o**

**Gap Esclarecido:** *"Como dividir o GDD sem destruir continuidade contextual?"*

**Decis√£o:** Chunking baseado em estrutura l√≥gica do documento (headers Markdown)

**Processo:**
1. Parser Markdown identifica headers (`#`, `##`, `###`)
2. Cada se√ß√£o/subse√ß√£o vira um chunk independente
3. Metadata preservada: `{section_name, level, parent_section, file_path}`
4. Chunks de tamanho vari√°vel (100-2000 tokens, dependendo do conte√∫do)

**Justificativa:**
- **Preserva Contexto Narrativo:** Biografia de personagem completa em um chunk, n√£o fragmentada
- **Simples de Implementar:** Parsing de Markdown √© trivial (bibliotecas prontas)
- **Alinhado com GDD Real:** GDDs s√£o naturalmente estruturados em se√ß√µes l√≥gicas

**Exemplo:**
```markdown
# Personagens

## Personagem: Aria Luminastra
### Biografia
[Texto completo da biografia] ‚Üí CHUNK 1

### Habilidades
[Lista de habilidades] ‚Üí CHUNK 2

## Personagem: Kael Sombravento
### Biografia
[Texto completo] ‚Üí CHUNK 3
```

**Refer√™ncia ao Contexto Original:**
> *"O paradigma de 'Garbage In, Garbage Out' √© intensificado em documentos complexos como GDDs, onde a fragmenta√ß√£o arbitr√°ria de texto (chunking) destr√≥i a continuidade contextual."* (linhas 20-21)

---

#### **2.2 Modelo de Embedding: Sentence-Transformers via HuggingFace**

**Gap Esclarecido:** *"Qual modelo usar entre OpenAI text-embedding-3-large, Cohere embed-v3, sentence-transformers, ou custom?"*

**Decis√£o:** HuggingFace Inference API com modelos Sentence-Transformers (ex: `all-MiniLM-L6-v2`, `paraphrase-multilingual-mpnet-base-v2`)

**Justificativa:**
- **Custo M√≠nimo:** Free tier do HuggingFace (~30k requests/m√™s, suficiente para MVP)
- **Qualidade Suficiente:** Modelos open-source s√£o adequados para validar conceito
- **Portabilidade:** Mesma interface para trocar modelos (OpenAI, Cohere) posteriormente
- **Self-hosting Futuro:** Modelos s√£o open-source, podem ser hospedados localmente se necess√°rio

**Configura√ß√£o:**
```typescript
// Embedding via HuggingFace Inference API
const EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'; // 384 dimens√µes
const HF_API_KEY = process.env.HUGGINGFACE_API_KEY;
```

**Migra√ß√£o Futura:** Se qualidade for insuficiente, migrar para OpenAI `text-embedding-3-large` (3072 dim, ~$0.13/1M tokens).

**Refer√™ncia ao Contexto Original:**
> *"Enquanto os modelos de bi-encoder (usados na recupera√ß√£o inicial) precisam comprimir o significado de um documento inteiro em um √∫nico vetor, perdendo nuances finas..."* (linhas 36-37)

**Esclarecimento:** No MVP, aceitamos "perda de nuances" dos bi-encoders. Adicionamos reranking (cross-encoders/ColBERT) apenas se encontrarmos problemas de precis√£o.

---

#### **2.3 Busca H√≠brida: pgvector + Postgres Full-Text Search (BM25)**

**Gap Esclarecido:** *"O documento menciona busca h√≠brida (Dense + Sparse) mas n√£o especifica implementa√ß√£o. Elasticsearch? Typesense? Built-in?"*

**Decis√£o:** Busca Vetorial (pgvector) + Full-Text Search nativo do Postgres, mesclados via Reciprocal Rank Fusion (RRF)

**Processo:**
1. **Query do usu√°rio** ‚Üí gera embedding via HuggingFace
2. **Query Vetorial (pgvector):**
   ```sql
   SELECT id, chunk_text, 1 - (embedding <=> query_embedding) AS similarity
   FROM gdd_chunks
   ORDER BY embedding <=> query_embedding
   LIMIT 20;
   ```
3. **Query Full-Text Search (Postgres FTS):**
   ```sql
   SELECT id, chunk_text, ts_rank(search_vector, plainto_tsquery('english', query_text)) AS rank
   FROM gdd_chunks
   WHERE search_vector @@ plainto_tsquery('english', query_text)
   ORDER BY rank DESC
   LIMIT 20;
   ```
4. **Reciprocal Rank Fusion (RRF):**
   ```typescript
   // Combina rankings de vetorial e FTS
   function rrf(vectorResults, ftsResults, k = 60) {
     const scores = new Map();
     vectorResults.forEach((doc, rank) => {
       scores.set(doc.id, (scores.get(doc.id) || 0) + 1 / (k + rank + 1));
     });
     ftsResults.forEach((doc, rank) => {
       scores.set(doc.id, (scores.get(doc.id) || 0) + 1 / (k + rank + 1));
     });
     return Array.from(scores.entries())
       .sort((a, b) => b[1] - a[1])
       .slice(0, 10); // Top 10 final
   }
   ```

**Justificativa:**
- **Best of Both Worlds:** Busca sem√¢ntica (vetorial) + precis√£o em nomes pr√≥prios/keywords (BM25)
- **Zero Depend√™ncias Externas:** Tudo implement√°vel em SQL puro no Postgres
- **Alinhado ao Documento:** Cobre o requisito de busca h√≠brida mencionado nas linhas 32-34

**Refer√™ncia ao Contexto Original:**
> *"O primeiro est√°gio foca em recall (abrang√™ncia), utilizando buscas h√≠bridas que combinam vetores densos (para significado sem√¢ntico) e vetores esparsos como BM25 (para precis√£o de palavras-chave t√©cnicas e nomes pr√≥prios)."* (linhas 32-34)

---

#### **2.4 Reranking: N√£o Implementado no MVP**

**Gap Esclarecido:** *"Usar ColBERT v2, Cross-Encoders (ms-marco-MiniLM, Cohere Rerank) ou LLM-as-a-reranker?"*

**Decis√£o:** **Nenhum reranking adicional no MVP**. Confia em busca h√≠brida + RRF.

**Justificativa:**
- **YAGNI (You Aren't Gonna Need It):** Adicionar reranking sem validar se a busca h√≠brida sozinha j√° resolve o problema √© otimiza√ß√£o prematura
- **Simplicidade:** Zero depend√™ncias extras, zero lat√™ncia adicional
- **Itera√ß√£o:** Se testes mostrarem resultados imprecisos (chunks irrelevantes no top-10), adicionar reranking incrementalmente

**Migra√ß√£o Futura (quando necess√°rio):**
1. **Fase 1:** Adicionar Cohere Rerank API (mais simples, ~$1 por 1000 reranks)
2. **Fase 2:** Se custo for alto, implementar ColBERT v2 self-hosted
3. **Fase 3:** Se lat√™ncia for cr√≠tica, usar cross-encoder local (ms-marco-MiniLM-L-6-v2)

**Refer√™ncia ao Contexto Original:**
> *"O reranking tornou-se o componente mais cr√≠tico para garantir a precis√£o final. (...) Em benchmarks de produ√ß√£o, o uso de cross-encoders elevou a m√©trica NDCG@10 em at√© 63%."* (linhas 35-38)

**Esclarecimento:** Esse √© o impacto em **produ√ß√£o otimizada**. No MVP, validamos se o problema existe antes de resolver.

---

### 3. LLMs, Guardrails e Prompting

#### **3.1 LLM para Gera√ß√£o: Claude 3.5 Sonnet (Anthropic)**

**Gap Esclarecido:** *"Qual LLM usar entre GPT-4 Turbo, Claude 3.5 Sonnet, Gemini Pro, ou modelos locais (Llama 3, Mistral)?"*

**Decis√£o:** Claude 3.5 Sonnet via Anthropic API

**Justificativa:**
- **Qualidade Superior para Narrativa:** Claude √© consistentemente melhor em manter coer√™ncia narrativa, worldbuilding e lore consistency (caso de uso exato do projeto)
- **Menos Alucina√ß√µes em RAG:** Benchmarks mostram Claude 3.5 com menor taxa de alucina√ß√µes em tarefas de Retrieval-Augmented Generation comparado a GPT-4o
- **Contexto MAIOR:** 200k tokens (vs 128k do GPT-4o) - permite passar 15-20 chunks + metadados extensos do grafo
- **Melhor Seguimento de Instru√ß√µes:** Excelente em aderir a system prompts complexos ("responda APENAS baseado no contexto")
- **Custo Fixo em Desenvolvimento:** Uso da API key do plano Claude Pro existente ($20/m√™s flat fee) - sem custos adicionais at√© atingir limite de ~150-200 mensagens/dia
- **Ecossistema Maduro:** SDK oficial (@anthropic-ai/sdk), documenta√ß√£o extensa, suporte a function calling

**Configura√ß√£o:**
```typescript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY, // API key do plano Claude Pro
});

const MODEL = 'claude-3-5-sonnet-20241022';
const MAX_TOKENS = 1000; // Respostas concisas
const TEMPERATURE = 0.3; // Baixa criatividade, alta ader√™ncia ao contexto
```

**Setup da API Key:**
```bash
# Obter API key do Claude CLI (plano Pro)
cat ~/.anthropic/api_key
# OU
cat ~/.config/claude/config.json

# Adicionar ao .env
ANTHROPIC_API_KEY=sk-ant-api03-xxx...
```

**Nota sobre Limite do Plano Pro:**
- Durante desenvolvimento/MVP com poucos usu√°rios, o limite de ~150-200 mensagens/dia do plano Pro √© suficiente
- Em produ√ß√£o com m√∫ltiplos designers, monitorar uso e considerar:
  - **Op√ß√£o A:** Continuar com plano Pro (se uso permanecer dentro do limite)
  - **Op√ß√£o B:** Criar API key separada pay-per-use para produ√ß√£o (~$3 input, $15 output por 1M tokens)

**Refer√™ncia ao Contexto Original:**
> *"Para garantir que as mec√¢nicas de RPG n√£o sofram distor√ß√µes, a stack de 2026 implementa camadas de verifica√ß√£o p√≥s-gera√ß√£o. Mesmo com uma recupera√ß√£o perfeita, os LLMs podem interpretar erroneamente datas, n√∫meros ou depend√™ncias l√≥gicas."* (linhas 68-70)

**Esclarecimento:** No MVP, confiamos em prompt engineering para evitar distor√ß√µes. Guardrails program√°ticos ser√£o adicionados se/quando encontrarmos problemas recorrentes.

---

#### **3.2 LLM para Extra√ß√£o de Entidades: Claude 3.5 Sonnet (mesmo modelo)**

**Gap Esclarecido:** *"Usar modelo separado (GPT-4o-mini, Claude Haiku) para extra√ß√£o ou reutilizar o mesmo LLM?"*

**Decis√£o:** Mesmo Claude 3.5 Sonnet usado para gera√ß√£o de respostas

**Justificativa:**
- **Simplicidade:** Uma API key (plano Pro), uma configura√ß√£o, consist√™ncia de qualidade
- **Custo Fixo:** Extra√ß√£o √© offline (script manual), conta no limite do plano Pro, zero custo adicional no MVP
- **Qualidade Superior:** Claude 3.5 Sonnet √© excelente em extra√ß√£o estruturada, especialmente para conte√∫do narrativo complexo
- **Contexto Grande:** 200k tokens permite processar se√ß√µes grandes do GDD de uma vez
- **Estrutura√ß√£o JSON:** Suporte nativo a respostas estruturadas via prompt engineering

**Processo de Extra√ß√£o:**
```python
# Script: ingest-gdd.py
import anthropic

client = anthropic.Anthropic(
    api_key=os.environ.get("ANTHROPIC_API_KEY")  # API key do plano Pro
)

def extract_entities(section_text: str) -> dict:
    prompt = f"""Extract narrative entities from this GDD section.
Return ONLY valid JSON with entities and relations.

Entities to extract: Personagem, Fac√ß√£o, Localiza√ß√£o, Evento, Lore, Miss√£o, Relacionamento, Arco, Tema
Relations to identify: PERTENCE_A, LOCALIZADO_EM, TEM_RELACIONAMENTO, EVOLUI_EM, EXPLORA_TEMA, etc.

Section:
{section_text}

Return JSON format:
{{
  "entities": [
    {{"type": "Personagem", "name": "...", "description": "..."}},
    ...
  ],
  "relations": [
    {{"source": "...", "type": "PERTENCE_A", "target": "..."}},
    ...
  ]
}}"""

    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )

    return json.loads(response.content[0].text)
```

**Nota sobre Uso:**
- Ingest√£o manual/offline conta no limite di√°rio do plano Pro
- Para GDD grande (~50 p√°ginas), estimar ~20-30 chamadas de extra√ß√£o
- Se atingir limite, processar em m√∫ltiplos dias ou considerar API key pay-per-use separada apenas para ingest√£o batch

---

#### **3.2.1 Configura√ß√£o da API Key do Plano Claude Pro**

**Como Obter a API Key:**

A API key usada pelo Claude CLI (plano Pro) pode ser reutilizada na aplica√ß√£o NestJS. Existem duas maneiras de obter:

**Op√ß√£o 1: Extrair do Claude CLI (Mac):**
```bash
# Localizar arquivo de configura√ß√£o do Claude CLI
cat ~/.anthropic/api_key
# OU
cat ~/.config/claude/config.json | jq -r '.api_key'
# OU
cat ~/Library/Application\ Support/Claude/config.json | jq -r '.api_key'
```

**Op√ß√£o 2: Gerar Nova no Console Anthropic:**
```bash
# Acessar: https://console.anthropic.com/settings/keys
# Criar nova API key (associada ao mesmo plano Pro)
# Copiar key no formato: sk-ant-api03-xxx...
```

**Configurar no Projeto:**

```bash
# .env (desenvolvimento local)
ANTHROPIC_API_KEY=sk-ant-api03-xxx...
HUGGINGFACE_API_KEY=hf_xxx...
DATABASE_URL=postgresql://sentinel:password@localhost:5432/sentinel_gdd
```

```bash
# .env.example (versionar no Git)
ANTHROPIC_API_KEY=
HUGGINGFACE_API_KEY=
DATABASE_URL=
```

**Instala√ß√£o do SDK:**
```bash
# NestJS
npm install @anthropic-ai/sdk

# Python (script de ingest√£o)
pip install anthropic
```

**Uso no NestJS:**
```typescript
// src/modules/gdd-rag/services/llm.service.ts
import Anthropic from '@anthropic-ai/sdk';
import { Injectable } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';

@Injectable()
export class LlmService {
  private anthropic: Anthropic;

  constructor(private configService: ConfigService) {
    this.anthropic = new Anthropic({
      apiKey: this.configService.get<string>('ANTHROPIC_API_KEY'),
    });
  }

  async generateResponse(systemPrompt: string, userPrompt: string): Promise<string> {
    const response = await this.anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1000,
      temperature: 0.3,
      system: systemPrompt,
      messages: [
        {
          role: 'user',
          content: userPrompt,
        },
      ],
    });

    return response.content[0].text;
  }
}
```

**Verificar Limite do Plano Pro:**

```bash
# O plano Pro tem limite de ~150-200 mensagens/dia (compartilhado entre CLI e API)
# Monitorar uso via dashboard: https://console.anthropic.com/settings/usage

# Durante MVP com poucos usu√°rios, o limite √© suficiente
# Se ultrapassar, considerar:
# 1. Otimizar prompts (reduzir chamadas redundantes)
# 2. Cache de respostas comuns
# 3. API key pay-per-use separada para produ√ß√£o
```

**Deploy em Produ√ß√£o (VPS):**

```bash
# SSH no VPS
ssh user@seu-vps

# Configurar .env no servidor
cd /var/www/sentinel
nano .env

# Adicionar ANTHROPIC_API_KEY (mesma do plano Pro)
ANTHROPIC_API_KEY=sk-ant-api03-xxx...

# Restart aplica√ß√£o
pm2 restart sentinel
```

**Importante:** A API key do plano Pro funciona em qualquer ambiente (local, Docker, VPS, cloud). O limite de mensagens √© global (soma de todas as chamadas), n√£o por ambiente.

---

#### **3.3 Framework de Guardrails: Valida√ß√£o via Prompt Engineering**

**Gap Esclarecido:** *"Implementar guardrails em 3 n√≠veis (Determin√≠stico + ML + LLM Supervisor) como descrito no documento, ou come√ßar simples?"*

**Decis√£o:** **Prompt Engineering** no MVP. Sem frameworks complexos (Guardrails AI, NeMo Guardrails) ou LLM Supervisor.

**System Prompt:**
```typescript
const SYSTEM_PROMPT = `
Voc√™ √© um assistente especializado em Documentos de Game Design (GDD) de RPG.

REGRAS CR√çTICAS:
1. Responda APENAS baseado no contexto fornecido abaixo (chunks de texto + metadados do grafo).
2. Se a informa√ß√£o n√£o estiver no contexto, diga: "N√£o encontrei essa informa√ß√£o no GDD fornecido."
3. NUNCA invente nomes de personagens, localiza√ß√µes, eventos ou lore que n√£o estejam no contexto.
4. Se houver ambiguidade ou contradi√ß√£o no contexto, mencione explicitamente.
5. Mantenha respostas concisas (m√°ximo 3-4 par√°grafos).

O contexto ser√° fornecido em dois formatos:
- TEXT CHUNKS: Trechos narrativos do GDD
- GRAPH METADATA: Entidades e rela√ß√µes estruturadas extra√≠das do grafo de conhecimento
`;
```

**Valida√ß√µes Futuras (quando necess√°rio):**
1. **Determin√≠sticas:** Regex para detectar "como IA eu..." ou disclaimers indesejados
2. **Schema Validation:** Se resposta for JSON estruturado (lista de personagens), validar schema
3. **LLM Supervisor:** Passar resposta + contexto para Claude Haiku (modelo menor/r√°pido) ou mesmo Claude 3.5 Sonnet avaliar groundedness

**Justificativa:**
- **Itera√ß√£o R√°pida:** Testar prompts √© instant√¢neo; implementar Guardrails AI leva dias
- **Custo Zero no MVP:** Valida√ß√£o via prompts n√£o tem custo adicional
- **Suficiente para MVP:** Claude 3.5 Sonnet √© excelente em seguir instru√ß√µes quando bem promptado

**Refer√™ncia ao Contexto Original:**
> *"A arquitetura de guardrails √© dividida em tr√™s n√≠veis de defesa: Validadores de Regras (Determin√≠sticos), Classificadores de ML, Validadores Sem√¢nticos baseados em LLM."* (linhas 69-73)

**Esclarecimento:** Essa √© a arquitetura de produ√ß√£o madura. MVP come√ßa com n√≠vel 0 (prompts) e adiciona n√≠veis conforme necess√°rio.

---

#### **3.4 Estrat√©gia de Prompting: Texto + Metadados do Grafo**

**Gap Esclarecido:** *"Passar apenas chunks de texto (RAG tradicional) ou incluir metadados estruturados do grafo?"*

**Decis√£o:** **Prompt H√≠brido** - combina chunks textuais + metadados do grafo de conhecimento

**Template de Prompt:**
```typescript
function buildPrompt(query: string, chunks: Chunk[], graphMetadata: GraphData): string {
  return `
${SYSTEM_PROMPT}

===== KNOWLEDGE GRAPH METADATA =====
Entidades Relevantes:
${graphMetadata.entities.map(e => `- ${e.type}: ${e.name} (${e.description})`).join('\n')}

Rela√ß√µes Relevantes:
${graphMetadata.relations.map(r => `- ${r.source} ${r.type} ${r.target}`).join('\n')}

===== TEXT CHUNKS FROM GDD =====
${chunks.map((c, i) => `
[Chunk ${i+1}] Se√ß√£o: ${c.section_name}
${c.text}
`).join('\n---\n')}

===== USER QUERY =====
${query}

Responda a query do usu√°rio usando APENAS o contexto acima.
`;
}
```

**Exemplo de Execu√ß√£o:**
```
User Query: "Quais personagens t√™m rivalidade com a Fac√ß√£o do Crep√∫sculo?"

KNOWLEDGE GRAPH METADATA:
Entidades Relevantes:
- Personagem: Kael Sombravento (guerreiro exilado)
- Personagem: Aria Luminastra (maga da Ordem da Luz)
- Fac√ß√£o: Fac√ß√£o do Crep√∫sculo (culto sombrio)
- Relacionamento: Rivalidade (√≥dio profundo)

Rela√ß√µes Relevantes:
- Kael Sombravento TEM_RELACIONAMENTO(Rivalidade) Fac√ß√£o do Crep√∫sculo
- Aria Luminastra CONFLITA_COM Fac√ß√£o do Crep√∫sculo

TEXT CHUNKS FROM GDD:
[Chunk 1] Se√ß√£o: Personagens > Kael Sombravento > Biografia
Kael foi exilado de sua terra natal ap√≥s descobrir que a Fac√ß√£o do Crep√∫sculo...
[texto completo]

[Chunk 2] Se√ß√£o: Fac√ß√µes > Fac√ß√£o do Crep√∫sculo
A Fac√ß√£o do Crep√∫sculo √© um culto que busca...
[texto completo]
```

**Justificativa:**
- **Aproveita o Grafo:** N√£o basta ter o grafo; o LLM precisa VER as conex√µes estruturadas
- **Queries Relacionais:** Perguntas como "quem tem rivalidade com X?" s√£o respondidas diretamente pelas rela√ß√µes do grafo
- **Contexto Duplo:** Texto narrativo (chunks) + estrutura sem√¢ntica (grafo) = m√°xima fidelidade

**Refer√™ncia ao Contexto Original:**
> *"O GraphRAG surge como a t√©cnica dominante em 2026 para lidar com a descoberta de informa√ß√µes em dados narrativos privados e t√©cnicos. Ao contr√°rio do RAG de linha de base, que tem dificuldade em conectar pontos dispersos em grandes cole√ß√µes de documentos, o GraphRAG utiliza LLMs para criar grafos de conhecimento que facilitam o entendimento de conceitos sem√¢nticos resumidos."* (linhas 25-29)

---

### 4. Infraestrutura e Deploy

#### **4.1 Setup do PostgreSQL: Docker Compose + Imagem Custom**

**Gap Esclarecido:** *"Usar Postgres managed cloud (AWS RDS, Supabase), VM custom, ou Docker local?"*

**Decis√£o:** Docker Compose com imagem customizada (Postgres 16 + Apache AGE + pgvector)

**Dockerfile (Postgres Custom):**
```dockerfile
FROM postgres:16

# Instala depend√™ncias de compila√ß√£o
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    postgresql-server-dev-16

# Instala Apache AGE
RUN cd /tmp && \
    git clone https://github.com/apache/age.git && \
    cd age && \
    make install

# Instala pgvector
RUN cd /tmp && \
    git clone https://github.com/pgvector/pgvector.git && \
    cd pgvector && \
    make && make install

# Cleanup
RUN apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/*
```

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  postgres:
    build: ./postgres-custom
    container_name: sentinel-postgres
    environment:
      POSTGRES_USER: sentinel
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: sentinel_gdd
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    command: postgres -c shared_preload_libraries=age

  nestjs:
    build: .
    container_name: sentinel-api
    environment:
      DATABASE_URL: postgresql://sentinel:${DB_PASSWORD}@postgres:5432/sentinel_gdd
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      HUGGINGFACE_API_KEY: ${HUGGINGFACE_API_KEY}
    ports:
      - "3000:3000"
    depends_on:
      - postgres

volumes:
  postgres_data:
```

**init.sql:**
```sql
-- Habilita extens√µes
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS age;

-- Cria schema do AGE
SELECT create_graph('gdd_graph');

-- Cria tabela de chunks com vetores
CREATE TABLE gdd_chunks (
    id SERIAL PRIMARY KEY,
    section_name TEXT NOT NULL,
    section_level INT,
    chunk_text TEXT NOT NULL,
    embedding vector(384), -- Sentence-Transformers dimension
    search_vector tsvector, -- Full-Text Search
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

-- √çndices
CREATE INDEX ON gdd_chunks USING ivfflat (embedding vector_cosine_ops);
CREATE INDEX ON gdd_chunks USING gin(search_vector);
```

**Justificativa:**
- **Reproduz√≠vel:** Qualquer dev roda `docker-compose up` e tem ambiente completo
- **Port√°vel:** Deploy em qualquer cloud que rode Docker (Render, Railway, AWS ECS, GCP Cloud Run)
- **Zero Custo Inicial:** Desenvolvimento local gratuito
- **Infra-as-Code:** `docker-compose.yml` versionado no Git

**Refer√™ncia ao Contexto Original:**
> *"A escolha do protocolo de transporte entre o BFF em NestJS e o m√≥dulo de IA √© determinante para a lat√™ncia total do sistema."* (linhas 54-56)

**Esclarecimento:** N√£o h√° "m√≥dulo de IA separado" no monolito. Toda comunica√ß√£o √© in-process (chamadas TypeScript ‚Üí APIs externas via HTTP).

---

#### **4.2 Deploy da Aplica√ß√£o: VPS Tradicional (DigitalOcean/Linode/EC2)**

**Gap Esclarecido:** *"Deploy via PaaS (Render/Railway), Kubernetes, Serverless (Vercel/Lambda), ou VPS tradicional?"*

**Decis√£o:** VPS tradicional com Git + PM2 + Nginx

**Setup (Ubuntu 22.04 LTS):**
```bash
# 1. Instala depend√™ncias
sudo apt update
sudo apt install -y nodejs npm nginx docker.io docker-compose git

# 2. Clona reposit√≥rio
git clone https://github.com/seu-user/sentinel.git /var/www/sentinel
cd /var/www/sentinel

# 3. Configura ambiente
cp .env.example .env
nano .env # Edita API keys, DB password

# 4. Inicia Postgres via Docker Compose
docker-compose up -d postgres

# 5. Build NestJS
npm install
npm run build

# 6. Configura PM2
npm install -g pm2
pm2 start dist/main.js --name sentinel
pm2 save
pm2 startup # Configura auto-start

# 7. Configura Nginx como reverse proxy
sudo nano /etc/nginx/sites-available/sentinel
```

**nginx.conf:**
```nginx
server {
    listen 80;
    server_name seu-dominio.com;

    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

**Deploy de Updates:**
```bash
# SSH no servidor
ssh user@seu-vps

# Pull c√≥digo novo
cd /var/www/sentinel
git pull origin main

# Rebuild (se deps mudaram)
npm install
npm run build

# Restart
pm2 restart sentinel
```

**Justificativa:**
- **Controle Total:** Acesso root, configura tudo manualmente
- **Custo Fixo Previs√≠vel:** $10-40/m√™s (vs PaaS que pode escalar inesperadamente)
- **Aprendizado Hands-on:** Entende cada camada (Nginx, PM2, Docker, Node)
- **Sem Lock-in:** Migra√ß√£o f√°cil entre providers (DigitalOcean ‚Üí Linode ‚Üí AWS EC2)

**Trade-off Aceito:** Responsabilidade operacional (backups, SSL, updates, monitoramento). Mitigado por scripts de automa√ß√£o futuros.

---

#### **4.3 Autentica√ß√£o: Sem Autentica√ß√£o (Desenvolvimento Local/Interno)**

**Gap Esclarecido:** *"API Key, JWT, OAuth, ou sem autentica√ß√£o?"*

**Decis√£o:** **Sem autentica√ß√£o no MVP** (desenvolvimento local/interno)

**Configura√ß√£o Atual:**
```typescript
// NestJS Controller
@Controller('api/rag')
export class RagController {
  @Post('query')
  async query(@Body() body: { query: string }) {
    // Sem valida√ß√£o de auth
    return this.ragService.processQuery(body.query);
  }
}
```

**‚ö†Ô∏è IMPORTANTE:**
- **Ambiente Atual:** NestJS roda em `localhost:3000` ou rede interna do VPS (n√£o exposto publicamente)
- **Antes de Deploy P√∫blico:** Implementar API Key b√°sica:
  ```typescript
  @UseGuards(ApiKeyGuard)
  @Controller('api/rag')
  export class RagController { ... }
  ```

**Migra√ß√£o Futura:**
1. **Fase 1 (pr√©-deploy p√∫blico):** API Key via header `X-API-Key`
2. **Fase 2 (m√∫ltiplos usu√°rios):** JWT com login (NestJS Passport)
3. **Fase 3 (integra√ß√£o SSO):** OAuth2 (Google/Microsoft)

**Justificativa:**
- **MVP Interno:** Equipe pequena, ambiente controlado, fric√ß√£o zero durante desenvolvimento
- **Itera√ß√£o R√°pida:** Testa funcionalidades sem overhead de auth
- **Seguran√ßa Adequada:** Se n√£o expor porta publicamente, n√£o h√° risco

---

### 5. MVP e Roadmap

#### **5.1 Escopo do MVP: Query & Answer + Explora√ß√£o de Entidades**

**Gap Esclarecido:** *"Quais features s√£o MUST-HAVE vs NICE-TO-HAVE no MVP?"*

**Decis√£o:** MVP com 2 funcionalidades core

**Features MUST-HAVE:**

1. **Query & Answer (RAG Conversacional)**
   - **Endpoint:** `POST /api/rag/query`
   - **Input:** `{ "query": "Quem √© o Personagem X?" }`
   - **Output:** Resposta narrativa baseada em chunks + grafo
   - **Pipeline Completo:** Embedding ‚Üí Busca H√≠brida ‚Üí Consulta Grafo ‚Üí Prompt Claude 3.5 Sonnet

2. **Explora√ß√£o de Entidades**
   - **Endpoints:**
     - `GET /api/rag/entities?type=Personagem` (listar entidades por tipo)
     - `GET /api/rag/entities/:id` (detalhes de uma entidade espec√≠fica)
     - `GET /api/rag/relations?entity=:id` (rela√ß√µes de uma entidade)
   - **Queries Exemplo:**
     - "Listar todos os personagens"
     - "Ver detalhes do Personagem 'Aria Luminastra'"
     - "Quais entidades t√™m rela√ß√£o 'RIVALIDADE' com Fac√ß√£o X?"

**Features NICE-TO-HAVE (p√≥s-MVP):**
- Interface web (React/Vue)
- Hist√≥rico de conversa√ß√£o (follow-up questions)
- Visualiza√ß√£o do grafo (D3.js, vis.js)
- Cita√ß√µes autom√°ticas de se√ß√µes do GDD
- Export de respostas (PDF/Markdown)

**Justificativa:**
- **Valida√ß√£o Dual:** Testa tanto RAG conversacional (queries livres) quanto navega√ß√£o estrutural (explora√ß√£o do grafo)
- **Feedback de Extra√ß√£o:** Designers veem as entidades extra√≠das, podem validar qualidade
- **Interface Simples:** Test√°vel via Postman/curl, n√£o requer frontend

**Refer√™ncia ao Contexto Original:**
> *"Casos reais, como a implementa√ß√£o de grafos orientados a objetivos (GoGs) no ambiente de Minecraft, demonstram que ao modelar explicitamente as depend√™ncias l√≥gicas entre sub-objetivos, a capacidade de racioc√≠nio da IA em tarefas de jogo supera significativamente as abordagens de RAG fragmentadas."* (linhas 86-88)

**Esclarecimento:** Explora√ß√£o de entidades permite validar se as "depend√™ncias l√≥gicas" (rela√ß√µes do grafo) foram extra√≠das corretamente.

---

#### **5.2 Dados Iniciais: GDD Real do Projeto Sentinel**

**Gap Esclarecido:** *"Criar GDD sint√©tico para testes ou usar GDD real desde o in√≠cio?"*

**Decis√£o:** **GDD Real do Projeto Sentinel**

**Processo de Ingest√£o (Manual):**
```bash
# 1. Localizar GDD do Sentinel
# Assumindo estrutura: docs/gdd/

# 2. Executar script de ingest√£o
python scripts/ingest-gdd.py --file docs/gdd/narrative.md

# Script executa:
# - Parse Markdown (chunking por se√ß√£o)
# - Gera embeddings via HuggingFace
# - Extrai entidades/rela√ß√µes via GPT-4o
# - Popula Postgres (chunks + grafo)
# - Exibe summary: "Extra√≠das 15 entidades, 23 rela√ß√µes, 42 chunks"
```

**Valida√ß√£o P√≥s-Ingest√£o:**
```bash
# Query SQL para verificar
psql -U sentinel -d sentinel_gdd -c "SELECT COUNT(*) FROM gdd_chunks;"
psql -U sentinel -d sentinel_gdd -c "SELECT * FROM cypher('gdd_graph', $$ MATCH (n) RETURN n LIMIT 10 $$) as (result agtype);"
```

**Justificativa:**
- **Realismo Imediato:** Descobre problemas de parsing, extra√ß√£o e estrutura com dados reais
- **Value Proposition Claro:** Designers testam com SEUS pr√≥prios GDDs, n√£o exemplos gen√©ricos
- **Feedback Qualitativo:** "O sistema entendeu o Personagem X corretamente?" √© pergunta real, n√£o hipot√©tica

**Trade-off Aceito:** GDD real pode ter inconsist√™ncias/estrutura irregular. Isso √© **bom** - for√ßa o sistema a ser robusto.

---

#### **5.3 Integra√ß√£o com Sentinel: M√≥dulo Isolado**

**Gap Esclarecido:** *"Integrar profundamente com arquitetura existente do Sentinel ou manter isolado?"*

**Decis√£o:** **M√≥dulo Isolado** (`src/modules/gdd-rag/`)

**Estrutura de Pastas:**
```
sentinel/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/          # M√≥dulos existentes do Sentinel
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gdd-rag/       # NOVO M√ìDULO ISOLADO
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ rag-query.controller.ts
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ rag-entities.controller.ts
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ rag.service.ts
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ embedding.service.ts
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ search.service.ts (pgvector + FTS + RRF)
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ graph.service.ts (Apache AGE queries)
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ llm.service.ts (Anthropic Claude 3.5 Sonnet)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ gdd.repository.ts
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dto/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ query.dto.ts
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ entity.dto.ts
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ gdd-rag.module.ts
‚îÇ   ‚îî‚îÄ‚îÄ app.module.ts
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ ingest-gdd.py      # Script Python de ingest√£o (offline)
‚îî‚îÄ‚îÄ docker-compose.yml
```

**gdd-rag.module.ts:**
```typescript
import { Module } from '@nestjs/common';
import { RagQueryController, RagEntitiesController } from './controllers';
import { RagService, EmbeddingService, SearchService, GraphService, LlmService } from './services';

@Module({
  controllers: [RagQueryController, RagEntitiesController],
  providers: [RagService, EmbeddingService, SearchService, GraphService, LlmService],
  exports: [], // N√£o exporta para outros m√≥dulos (isolado)
})
export class GddRagModule {}
```

**app.module.ts:**
```typescript
import { Module } from '@nestjs/common';
import { GddRagModule } from './modules/gdd-rag/gdd-rag.module';
// Outros m√≥dulos existentes...

@Module({
  imports: [
    // M√≥dulos existentes do Sentinel
    AuthModule,
    UsersModule,
    // Novo m√≥dulo isolado
    GddRagModule,
  ],
})
export class AppModule {}
```

**Justificativa:**
- **Zero Risco:** N√£o quebra funcionalidades existentes do Sentinel
- **Desenvolvimento Independente:** N√£o precisa entender toda arquitetura do Sentinel para come√ßar
- **Remo√ß√£o F√°cil:** Se MVP falhar, deletar pasta `gdd-rag/` remove tudo
- **Evolu√ß√£o Futura:** Quando est√°vel, pode refatorar para reutilizar infraestrutura existente (logging, auth, etc.)

**Endpoints Expostos:**
```
POST   /api/rag/query           # Query conversacional
GET    /api/rag/entities        # Listar entidades
GET    /api/rag/entities/:id    # Detalhes de entidade
GET    /api/rag/relations       # Explorar rela√ß√µes
```

---

## üìà Roadmap de Implementa√ß√£o

### **Fase 1: MVP Core (2-3 semanas)**

**Objetivo:** Pipeline completo funcionando end-to-end com GDD real

**Tarefas:**
1. ‚úÖ Setup Postgres via Docker Compose (AGE + pgvector)
2. ‚úÖ Script Python de ingest√£o (`ingest-gdd.py`)
   - Parser Markdown (chunking por se√ß√£o)
   - Integra√ß√£o HuggingFace (embeddings)
   - Integra√ß√£o Anthropic (extra√ß√£o de entidades via Claude 3.5 Sonnet)
   - Popular Postgres (chunks + grafo)
3. ‚úÖ M√≥dulo NestJS `gdd-rag`
   - `EmbeddingService` (HuggingFace API)
   - `SearchService` (pgvector + FTS + RRF)
   - `GraphService` (Apache AGE queries)
   - `LlmService` (Anthropic Claude 3.5 Sonnet)
   - `RagService` (orquestra pipeline sequencial)
4. ‚úÖ Controllers REST
   - `POST /api/rag/query`
   - `GET /api/rag/entities`
   - `GET /api/rag/entities/:id`
   - `GET /api/rag/relations`
5. ‚úÖ Deploy no VPS (Git + PM2 + Nginx)
6. ‚úÖ Testes com designers (feedback qualitativo)

**M√©tricas de Sucesso:**
- [ ] Ingest√£o de GDD real sem erros (>90% das entidades extra√≠das corretamente)
- [ ] Query response time <2s P95
- [ ] Designers reportam "respostas √∫teis" em >70% das queries

---

### **Fase 2: Refinamento & Guardrails (1-2 semanas)**

**Objetivo:** Melhorar qualidade e adicionar valida√ß√µes b√°sicas

**Tarefas:**
1. ‚úÖ Implementar cita√ß√µes de se√ß√µes no prompt (rastreabilidade)
2. ‚úÖ Adicionar valida√ß√µes determin√≠sticas (regex para disclaimers indesejados)
3. ‚úÖ Logging estruturado (query, chunks retornados, resposta, lat√™ncia)
4. ‚úÖ M√©tricas de uso (queries/dia, entidades mais consultadas)
5. ‚úÖ Melhorar prompts baseado em feedback (itera√ß√£o de system prompt)
6. ‚úÖ Otimizar chunking se necess√°rio (testar overlap, hierarquia)

**M√©tricas de Sucesso:**
- [ ] Alucina√ß√µes detectadas <5% das queries
- [ ] Designers conseguem rastrear fonte da resposta (cita√ß√µes funcionam)

---

### **Fase 3: Features de Produtividade (2-3 semanas)**

**Objetivo:** Transformar MVP em ferramenta di√°ria dos designers

**Tarefas:**
1. ‚úÖ Interface web b√°sica (React ou Vue)
   - Chat conversacional
   - Navega√ß√£o de entidades (cards, filtros)
2. ‚úÖ Hist√≥rico de conversa√ß√£o (session management)
3. ‚úÖ Autentica√ß√£o (API Key ou JWT)
4. ‚úÖ Visualiza√ß√£o do grafo (D3.js ou vis.js)
5. ‚úÖ Export de respostas (Markdown, PDF)
6. ‚úÖ Ingest√£o com preview (designer aprova entidades extra√≠das antes de commitar)

**M√©tricas de Sucesso:**
- [ ] >50% dos designers usam a ferramenta semanalmente
- [ ] Tempo m√©dio de busca no GDD reduz de 10min ‚Üí 2min

---

### **Fase 4: Otimiza√ß√£o & Escala (ongoing)**

**Objetivo:** Preparar para produ√ß√£o e escala

**Tarefas:**
1. ‚úÖ Adicionar reranking (Cohere Rerank API ou ColBERT)
2. ‚úÖ Implementar guardrails avan√ßados (Guardrails AI framework)
3. ‚úÖ CI/CD (GitHub Actions ‚Üí deploy autom√°tico)
4. ‚úÖ Monitoramento (Grafana, logs centralizados)
5. ‚úÖ Backups autom√°ticos do Postgres
6. ‚úÖ Migrar embeddings para OpenAI ou Voyage AI se qualidade do HF for insuficiente
7. ‚úÖ Avaliar microservi√ßos se lat√™ncia virar bottleneck
8. ‚úÖ Expans√£o para mec√¢nicas RPG (adicionar entidades: Classe, Habilidade, Item, Atributo)

**M√©tricas de Sucesso:**
- [ ] Uptime >99%
- [ ] Query response time <500ms P95
- [ ] Custo <$100/m√™s para 100 designers

---

## üîç Ambiguidades/Gaps Resolvidos

### **Do Documento de Pesquisa para Decis√µes Execut√°veis:**

| # | **Ambiguidade Original** | **Trecho do Documento (linhas)** | **Decis√£o Tomada** | **Como Foi Esclarecido** |
|---|--------------------------|----------------------------------|---------------------|--------------------------|
| 1 | Qual banco de grafos usar? | "grafos de conhecimento orientados por ontologias" (linha 22) | **Apache AGE** (Postgres extension) | Entrevista Rodada 1, Pergunta 1.1: Escolhido por unifica√ß√£o de infraestrutura (grafos + vetores + relacional no mesmo DB) |
| 2 | Qual vector database? | "busca vetorial" mencionada mas sem especifica√ß√£o (linha 33) | **pgvector** (Postgres extension) | Entrevista Rodada 1, Pergunta 1.2: Coes√£o arquitetural, zero custo extra, queries cross-domain |
| 3 | Quais entidades da ontologia RPG? | "classes de entidades (ex: Personagem, Magia, Atributo)" (linha 21) | **Ontologia Narrativa:** Personagem, Fac√ß√£o, Localiza√ß√£o, Evento, Lore, Miss√£o, Relacionamento, Arco, Tema | Entrevista Rodada 1, Perguntas 1.3-1.4: Foco narrativo-first, expandir para mec√¢nicas depois |
| 4 | Quais rela√ß√µes modelar? | "rela√ß√µes permitidas entre elas" (linha 21) | **Estruturais + Temporais + Emocionais:** PERTENCE_A, TEM_RELACIONAMENTO, ACONTECE_ANTES, EVOLUI_EM, etc. | Entrevista Rodada 1, Pergunta 1.4: Conjunto completo para suportar queries narrativas complexas |
| 5 | Como chunkar o GDD? | "fragmenta√ß√£o arbitr√°ria de texto (chunking) destr√≥i a continuidade" (linhas 20-21) | **Chunking sem√¢ntico por se√ß√£o** (headers Markdown) | Entrevista Rodada 2, Pergunta 2.1: Preserva contexto narrativo, simples de implementar |
| 6 | Qual API de embeddings? | N√£o especificado | **HuggingFace Inference API** (Sentence-Transformers) | Entrevista Rodada 2, Pergunta 2.2: Custo m√≠nimo, free tier, qualidade suficiente para MVP |
| 7 | Implementar reranking? | "reranking tornou-se o componente mais cr√≠tico" (linha 35) | **N√£o no MVP** (apenas busca h√≠brida + RRF) | Entrevista Rodada 2, Pergunta 2.3: YAGNI - adicionar apenas se busca h√≠brida for insuficiente |
| 8 | Qual modelo de reranking? | "ColBERT v2, cross-encoders, LLM listwise" (linhas 39-46) | **N/A no MVP** (decis√£o futura: Cohere Rerank ‚Üí ColBERT ‚Üí Cross-encoder) | Entrevista Rodada 2, Pergunta 2.3: Progress√£o de complexidade conforme necess√°rio |
| 9 | gRPC vs NATS? | Tabela comparativa (linhas 61-65) mas sem decis√£o | **N/A** (arquitetura monol√≠tica, sem microservi√ßos) | Entrevista Rodada 1 (Abordagem): Monolito elimina comunica√ß√£o inter-servi√ßos |
| 10 | Usar Python/Rust h√≠brido? | "m√≥dulo poliglota, utilizando Python (...) integrando Rust" (linha 52) | **Script Python offline** (ingest√£o) + **NestJS puro** (runtime) | Entrevista Rodada 1 (Abordagem): Simplicidade, sem overhead de PyO3/Maturin no MVP |
| 11 | Qual LLM para gera√ß√£o? | N√£o especificado | **Claude 3.5 Sonnet** (Anthropic) | Entrevista Rodada 3, Pergunta 3.1: Melhor para narrativa, menos alucina√ß√µes em RAG, contexto 200k tokens, usa plano Pro existente |
| 12 | Qual LLM para extra√ß√£o? | N√£o especificado | **Mesmo Claude 3.5 Sonnet** | Entrevista Rodada 3, Pergunta 3.2: Simplicidade (mesma API key do plano Pro), qualidade superior em extra√ß√£o narrativa |
| 13 | Framework de guardrails? | "guardrails √© dividida em tr√™s n√≠veis" (linhas 69-73) | **Prompt engineering no MVP** (sem frameworks) | Entrevista Rodada 3, Pergunta 3.3: Itera√ß√£o r√°pida, adicionar camadas conforme necess√°rio |
| 14 | Como estruturar prompts? | N√£o especificado | **Prompt h√≠brido:** chunks textuais + metadados do grafo | Entrevista Rodada 3, Pergunta 3.4: Aproveita tanto narrativa quanto estrutura relacional |
| 15 | Como executar queries? | N√£o especificado | **Pipeline sequencial:** embedding ‚Üí busca h√≠brida ‚Üí grafo ‚Üí LLM | Ap√≥s Rodada 3: Simplicidade, debugging trivial, cada etapa independente |
| 16 | Como hospedar Postgres? | N√£o especificado | **Docker Compose** (imagem custom AGE + pgvector) | Entrevista Rodada 4, Pergunta 4.1: Reproduz√≠vel, port√°vel, zero custo inicial |
| 17 | Onde fazer deploy? | N√£o especificado | **VPS tradicional** (DigitalOcean/Linode/EC2) | Entrevista Rodada 4, Pergunta 4.2: Controle total, custo fixo, aprendizado hands-on |
| 18 | Autentica√ß√£o da API? | N√£o mencionado | **Sem autentica√ß√£o no MVP** (dev local/interno) | Entrevista Rodada 4, Pergunta 4.3: Itera√ß√£o r√°pida, adicionar API Key antes de deploy p√∫blico |
| 19 | Escopo do MVP? | N√£o definido | **Query & Answer + Explora√ß√£o de Entidades** | Entrevista Rodada 5, Pergunta 5.1: Valida RAG conversacional E qualidade de extra√ß√£o do grafo |
| 20 | Dados de teste? | N√£o especificado | **GDD real do Sentinel** | Entrevista Rodada 5, Pergunta 5.2: Valida√ß√£o com complexidade real desde dia 1 |
| 21 | Integra√ß√£o com Sentinel? | N√£o mencionado | **M√≥dulo isolado** (`src/modules/gdd-rag/`) | Entrevista Rodada 5, Pergunta 5.3: Zero risco, desenvolvimento independente |

---

## üìö Trechos Esclarecidos do Contexto Original

### **1. Arquitetura de Microservi√ßos vs Monolito**

**Trecho Original (linhas 48-53):**
> *"A stack tecnol√≥gica para sustentar este pipeline complexo em 2026 baseia-se em uma arquitetura de microservi√ßos orquestrada por NestJS, atuando como o Backend-for-Frontend (BFF). O NestJS √© ideal para esta fun√ß√£o devido √† sua capacidade de abstrair transportadores de comunica√ß√£o atrav√©s de interfaces can√¥nicas, permitindo a integra√ß√£o perfeita entre mensagens baseadas em eventos e chamadas de solicita√ß√£o-resposta. No entanto, o motor de IA pesado ‚Äî onde residem a gera√ß√£o de embeddings, a busca vetorial e o reranking ‚Äî √© delegado a um m√≥dulo especializado."*

**Esclarecimento:**
- Essa √© a **arquitetura final de produ√ß√£o** descrita no documento (2026 estado-da-arte)
- **Decis√£o para MVP:** Come√ßar com **arquitetura monol√≠tica** (NestJS √∫nico, APIs externas para LLM/embeddings)
- **Justificativa:** Validar proposta de valor antes de investir em microservi√ßos
- **Caminho de evolu√ß√£o:** Monolito ‚Üí Microservi√ßos quando lat√™ncia/custo justificar (n√£o no MVP)

---

### **2. GraphRAG e Travessia de M√∫ltiplos Saltos**

**Trecho Original (linhas 25-30):**
> *"O GraphRAG surge como a t√©cnica dominante em 2026 para lidar com a descoberta de informa√ß√µes em dados narrativos privados e t√©cnicos. Ao contr√°rio do RAG de linha de base, que tem dificuldade em conectar pontos dispersos em grandes cole√ß√µes de documentos, o GraphRAG utiliza LLMs para criar grafos de conhecimento que facilitam o entendimento de conceitos sem√¢nticos resumidos. O processo envolve a identifica√ß√£o de 'piv√¥s' ‚Äî n√≥s de entrada altamente relevantes ‚Äî seguida pela expans√£o da relev√¢ncia atrav√©s da travessia do grafo."*

**Esclarecimento:**
- **Implementa√ß√£o no MVP:** Pipeline sequencial que combina busca vetorial (chunks) + consulta ao grafo (entidades/rela√ß√µes)
- **Prompts H√≠bridos:** LLM recebe TANTO chunks textuais QUANTO metadados estruturados do grafo
- **Multi-hop:** Queries no grafo usam Cypher para traversar rela√ß√µes (ex: `MATCH (p:Personagem)-[:TEM_RELACIONAMENTO]->(r:Relacionamento)-[:COM]->(f:Fac√ß√£o)`)
- **Diferen√ßa do RAG tradicional:** N√£o s√≥ busca chunks similares, mas tamb√©m navega rela√ß√µes sem√¢nticas expl√≠citas

---

### **3. Reranking com Cross-Encoders e ColBERT**

**Trecho Original (linhas 35-46):**
> *"O reranking tornou-se o componente mais cr√≠tico para garantir a precis√£o final. (...) Em benchmarks de produ√ß√£o, o uso de cross-encoders elevou a m√©trica NDCG@10 em at√© 63% em compara√ß√£o com sistemas baseados apenas em buscas por palavras-chave. Entretanto, devido √† alta lat√™ncia dos cross-encoders, o mercado de 2026 consolidou o uso de modelos de Intera√ß√£o Tardia (Late Interaction), especificamente o ColBERT v2."*

**Esclarecimento:**
- **Decis√£o para MVP:** **N√ÉO implementar reranking** inicialmente (apenas busca h√≠brida pgvector + BM25 + RRF)
- **Justificativa:** Validar se o problema existe antes de resolver (YAGNI)
- **Roadmap de reranking (se necess√°rio):**
  1. Fase 1: Adicionar Cohere Rerank API (mais simples, ~$1/1000 reranks)
  2. Fase 2: Implementar ColBERT v2 self-hosted (custo zero, lat√™ncia ~50-100ms)
  3. Fase 3: Cross-encoder local para top-5 final (m√°xima precis√£o)
- **Benchmark mencionado (63% melhoria):** √â em **produ√ß√£o otimizada**, n√£o em MVP. Evita otimiza√ß√£o prematura.

---

### **4. Guardrails em 3 N√≠veis**

**Trecho Original (linhas 69-73):**
> *"A arquitetura de guardrails √© dividida em tr√™s n√≠veis de defesa:*
> *- Validadores de Regras (Determin√≠sticos): Verificam se o output adere a esquemas JSON estritos e se valores num√©ricos est√£o dentro de limites operacionais.*
> *- Classificadores de ML: Detectam tentativas de inje√ß√£o de prompt ou conte√∫dos t√≥xicos.*
> *- Validadores Sem√¢nticos baseados em LLM: Utilizam um LLM 'supervisor' (Critic) para comparar a resposta gerada com os documentos de origem, calculando a m√©trica de Groundedness (Aterramento)."*

**Esclarecimento:**
- **Decis√£o para MVP:** **N√≠vel 0** - Apenas prompt engineering bem estruturado
- **System Prompt:** Instrui explicitamente Claude 3.5 Sonnet a responder APENAS baseado no contexto, n√£o inventar informa√ß√µes
- **Progress√£o de guardrails (conforme necess√°rio):**
  - **N√≠vel 1 (Determin√≠stico):** Regex para detectar disclaimers indesejados ("como IA eu...", "n√£o tenho certeza...")
  - **N√≠vel 2 (Schema Validation):** Validar JSON se resposta for estruturada (lista de entidades, atributos)
  - **N√≠vel 3 (LLM Supervisor):** Passar resposta + contexto para Claude Haiku ou mesmo Claude 3.5 Sonnet avaliar groundedness
  - **N√≠vel 4 (Framework):** Guardrails AI ou NeMo Guardrails (produ√ß√£o madura)
- **Justificativa:** Claude 3.5 Sonnet tem taxa de alucina√ß√£o menor que GPT-4o em RAG; testa prompts antes de adicionar complexidade

---

### **5. Chunking e Continuidade Contextual**

**Trecho Original (linhas 19-21):**
> *"A base de um sistema de RAG de alta fidelidade para 2026 come√ßa na fase de ingest√£o. O paradigma de 'Garbage In, Garbage Out' √© intensificado em documentos complexos como GDDs, onde a fragmenta√ß√£o arbitr√°ria de texto (chunking) destr√≥i a continuidade contextual necess√°ria para entender mec√¢nicas de RPG."*

**Esclarecimento:**
- **Problema identificado:** Chunking arbitr√°rio (janelas de 512 tokens fixas) quebra narrativas no meio
- **Solu√ß√£o adotada:** **Chunking sem√¢ntico por se√ß√£o** (baseado em headers Markdown)
- **Exemplo pr√°tico:**
  ```markdown
  ## Personagem: Aria Luminastra
  ### Biografia
  Aria nasceu na cidade de L√∫men... [texto completo de 800 tokens]
  ‚Üí CHUNK √öNICO preserva biografia completa

  Chunking arbitr√°rio faria:
  [0-512 tokens: Aria nasceu... at√© metade da hist√≥ria]
  [512-1024 tokens: ...continua√ß√£o da hist√≥ria at√© habilidades]
  ‚Üí QUEBRA CONTEXTO
  ```
- **Metadata preservada:** `{section_name: "Personagens > Aria Luminastra > Biografia", level: 3}`
- **Trade-off:** Chunks de tamanho vari√°vel (100-2000 tokens) vs chunks fixos. Aceit√°vel pois preserva significado.

---

### **6. Busca H√≠brida (Dense + Sparse)**

**Trecho Original (linhas 32-34):**
> *"A precis√£o sem√¢ntica em 2026 √© alcan√ßada atrav√©s de um pipeline de recupera√ß√£o em m√∫ltiplos est√°gios. O primeiro est√°gio foca em recall (abrang√™ncia), utilizando buscas h√≠bridas que combinam vetores densos (para significado sem√¢ntico) e vetores esparsos como BM25 (para precis√£o de palavras-chave t√©cnicas e nomes pr√≥prios)."*

**Esclarecimento:**
- **Dense (pgvector):** Busca por similaridade sem√¢ntica (embedding da query vs embeddings dos chunks)
  - **Bom para:** Queries conceituais ("quais personagens s√£o corajosos?", "eventos tr√°gicos na hist√≥ria")
- **Sparse (BM25 via Postgres FTS):** Busca por keywords exatas
  - **Bom para:** Nomes pr√≥prios ("Fac√ß√£o do Crep√∫sculo"), termos t√©cnicos ("atributo de resist√™ncia ao fogo")
- **Reciprocal Rank Fusion (RRF):** Algoritmo de merge que combina rankings de ambas as buscas
  - **F√≥rmula:** `score(doc) = Œ£ 1/(k + rank_i)` onde `rank_i` √© a posi√ß√£o do doc na lista `i`
  - **Resultado:** Top-10 chunks que balanceiam sem√¢ntica + keywords
- **Implementa√ß√£o:** 100% SQL nativo no Postgres (sem depend√™ncias externas como Elasticsearch)

---

### **7. Ontologia como Base**

**Trecho Original (linhas 22-24):**
> *"Ao alinhar o grafo de conhecimento com uma ontologia extra√≠da de bancos de dados relacionais est√°veis do est√∫dio, reduz-se drasticamente o custo computacional de infer√™ncias repetidas de LLM e elimina-se a necessidade de pipelines complexos de fus√£o de ontologias."*

**Esclarecimento:**
- **Contexto do documento:** Est√∫dios grandes t√™m DBs relacionais com esquemas est√°veis (ex: tabela `characters`, `factions`, `items`)
- **Realidade do MVP:** N√£o temos "bancos de dados relacionais est√°veis". GDD est√° em Markdown.
- **Solu√ß√£o adotada:**
  1. **Definir ontologia manualmente** (entidades: Personagem, Fac√ß√£o, etc.; rela√ß√µes: TEM_RELACIONAMENTO, etc.)
  2. **Extrair entidades do GDD via LLM** (Claude 3.5 Sonnet com prompts estruturados)
  3. **Popular grafo** (Apache AGE) com entidades/rela√ß√µes extra√≠das
  4. **Refinar iterativamente** (designers validam/corrigem extra√ß√£o)
- **Benef√≠cio futuro:** Se Sentinel tiver DB relacional de game data, mapear ontologia para schema do DB (reduz custo de infer√™ncia)

---

## üéØ M√©tricas de Sucesso Definidas

### **MVP (Fase 1):**
- [ ] **Taxa de Sucesso de Ingest√£o:** >90% das entidades do GDD extra√≠das corretamente (valida√ß√£o manual)
- [ ] **Lat√™ncia de Query:** <2s P95 (embedding + busca + grafo + LLM)
- [ ] **Qualidade Percebida:** Designers reportam "respostas √∫teis" em >70% das queries (pesquisa qualitativa)

### **P√≥s-Refinamento (Fase 2):**
- [ ] **Taxa de Alucina√ß√£o:** <5% das queries (resposta cont√©m informa√ß√£o n√£o presente no GDD)
- [ ] **Rastreabilidade:** 100% das respostas t√™m cita√ß√µes de se√ß√µes do GDD (se implementado)
- [ ] **Cobertura:** >80% das queries encontram contexto relevante (n√£o retornam "n√£o encontrei")

### **Produ√ß√£o (Fase 4):**
- [ ] **Ado√ß√£o:** >50% dos designers usam semanalmente
- [ ] **Efici√™ncia:** Tempo m√©dio de busca manual no GDD reduz de 10min ‚Üí <2min
- [ ] **Uptime:** >99%
- [ ] **Lat√™ncia Otimizada:** <500ms P95
- [ ] **Custo:** <$100/m√™s para 100 designers ativos

---

## üöÄ Pr√≥ximos Passos Imediatos

### **Semana 1-2: Setup de Infraestrutura**
1. [ ] Criar branch `feature/gdd-rag` no repo Sentinel
2. [ ] Implementar `docker-compose.yml` (Postgres + AGE + pgvector)
3. [ ] Testar setup local (`docker-compose up -d`, verificar extens√µes)
4. [ ] Criar script `init.sql` (schema de chunks, grafo, √≠ndices)

### **Semana 2-3: Script de Ingest√£o**
1. [ ] Implementar `scripts/ingest-gdd.py`:
   - Parser Markdown (biblioteca `markdown-it-py` ou `mistune`)
   - Integra√ß√£o HuggingFace Inference API (embeddings)
   - Integra√ß√£o OpenAI API (extra√ß√£o de entidades via GPT-4o com function calling)
   - Popular Postgres (`psycopg2`, queries SQL + Cypher)
2. [ ] Testar com GDD real do Sentinel
3. [ ] Validar qualidade de extra√ß√£o (manual review das entidades)

### **Semana 3-4: M√≥dulo NestJS**
1. [ ] Criar `src/modules/gdd-rag/`
2. [ ] Implementar services:
   - `EmbeddingService` (HuggingFace API)
   - `SearchService` (pgvector + FTS + RRF)
   - `GraphService` (Apache AGE queries via node-postgres)
   - `LlmService` (Anthropic API - Claude 3.5 Sonnet)
   - `RagService` (orquestra pipeline)
3. [ ] Implementar controllers:
   - `POST /api/rag/query`
   - `GET /api/rag/entities`
4. [ ] Testar localmente (Postman/curl)

### **Semana 4: Deploy e Feedback**
1. [ ] Deploy no VPS (setup PM2 + Nginx)
2. [ ] Testes com 2-3 designers early adopters
3. [ ] Coletar feedback qualitativo
4. [ ] Iterar em prompts e chunking baseado em feedback

---

## üîó Refer√™ncias

- **Documento Original:** [Stack RAG Alta Fidelidade para GDDs.md](../pesquisas/Stack%20RAG%20Alta%20Fidelidade%20para%20GDDs.md)
- **Tecnologias Decididas:**
  - [Apache AGE](https://age.apache.org/) - Graph extension for PostgreSQL
  - [pgvector](https://github.com/pgvector/pgvector) - Vector similarity search for PostgreSQL
  - [HuggingFace Inference API](https://huggingface.co/inference-api) - Embeddings
  - [Claude 3.5 Sonnet](https://docs.anthropic.com/en/docs/models-overview) - LLM via Anthropic API
  - [NestJS](https://nestjs.com/) - Backend framework
  - [Sentence-Transformers](https://www.sbert.net/) - Embedding models

---

## üìù Conclus√£o

Este documento consolida **21 decis√µes t√©cnicas cr√≠ticas** que transformam o conhecimento estado-da-arte do documento de pesquisa em uma arquitetura execut√°vel para o projeto Sentinel.

**Princ√≠pios Norteadores:**
1. **Simplicidade Primeiro:** Monolito antes de microservi√ßos, prompts antes de guardrails complexos
2. **Valida√ß√£o R√°pida:** MVP em 3-4 semanas, testes com dados reais, feedback cont√≠nuo
3. **Evolu√ß√£o Incremental:** Cada decis√£o tem caminho claro de upgrade (MVP ‚Üí Refinamento ‚Üí Produ√ß√£o)
4. **Custo Consciente:** <$100/m√™s no MVP, escala de custos apenas com valida√ß√£o de valor

**Diferenciais da Abordagem:**
- **GraphRAG Narrativo:** Foco em worldbuilding e consist√™ncia de lore (n√£o mec√¢nicas de combate)
- **Infraestrutura Unificada:** Postgres √∫nico (grafos + vetores + relacional)
- **Autonomia Operacional:** Self-hosted (VPS + Docker), sem lock-in de cloud

O sistema est√° pronto para implementa√ß√£o. Pr√≥ximo passo: criar branch `feature/gdd-rag` e come√ßar o setup de infraestrutura.

---

**Documento gerado em:** 2026-01-29
**Autor:** Entrevista estruturada com decis√µes consensuais
**Status:** ‚úÖ Aprovado para implementa√ß√£o
