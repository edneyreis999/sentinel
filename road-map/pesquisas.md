# Sum√°rio para Pesquisa T√©cnica: Melhores Pr√°ticas de Implementa√ß√£o

Baseado nas decis√µes arquiteturais do projeto Sentinel, este documento identifica as tecnologias centrais e os cen√°rios espec√≠ficos que precisam de aprofundamento t√©cnico.

## üéØ Contexto do Sistema

**Objetivo:** Sistema RAG de alta fidelidade para GDDs narrativos de RPG
**Arquitetura:** Monolito NestJS com PostgreSQL unificado (grafos + vetores + relacional)
**Foco:** Simplicidade MVP, valida√ß√£o r√°pida, custo m√≠nimo (<$100/m√™s)

---

## üìö Tecnologias Escolhidas & T√≥picos de Pesquisa

### 1. **PostgreSQL + Apache AGE (Grafos de Conhecimento)**

**Cen√°rios:**
- Modelagem de ontologia narrativa (Personagem, Fac√ß√£o, Evento, Relacionamento, Arco, Tema)
- Queries multi-hop para racioc√≠nio relacional (`MATCH` paths complexos)
- Extra√ß√£o automatizada de entidades/rela√ß√µes de GDD via LLM

**T√≥picos para pesquisar:**
- [ ] **Setup e configura√ß√£o de Apache AGE no Postgres 16**
  - Compila√ß√£o e instala√ß√£o via Docker
  - Configura√ß√£o de `shared_preload_libraries`
  - Cria√ß√£o de grafos (`create_graph`)
- [ ] **Modelagem de ontologia em Cypher para AGE**
  - Schema design para entidades narrativas com propriedades JSONB
  - Tipos de relacionamentos (estruturais, temporais, emocionais)
  - Versionamento de GDD no grafo (rela√ß√£o `VERS√ÉO_DE`)
- [ ] **Queries Cypher eficientes para cen√°rios narrativos**
  - Travessia multi-hop: "Personagens relacionados indiretamente com Fac√ß√£o X"
  - Agrega√ß√µes: "Todos os Eventos que mencionam Localiza√ß√£o Y"
  - Path finding: "Cadeia de Relacionamentos entre Personagem A e B"
- [ ] **Performance tuning de AGE**
  - √çndices em propriedades de n√≥s/arestas
  - Materializa√ß√£o de subgrafos frequentes
  - Limites de profundidade de travessia (evitar explos√£o combinat√≥ria)
- [ ] **Integra√ß√£o AGE + NestJS via node-postgres**
  - Executar queries Cypher via `SELECT * FROM cypher(...)`
  - Parsing de resultados `agtype` para TypeScript
  - Pool de conex√µes e transa√ß√µes

---

### 2. **pgvector + Postgres Full-Text Search (Busca H√≠brida)**

**Cen√°rios:**
- Busca sem√¢ntica em chunks narrativos (biografias, lore)
- Precis√£o em nomes pr√≥prios/keywords t√©cnicos (BM25)
- Merge de rankings via Reciprocal Rank Fusion (RRF)

**T√≥picos para pesquisar:**
- [ ] **Setup pgvector com HuggingFace embeddings**
  - Instala√ß√£o de extens√£o `vector`
  - Tipos de √≠ndice: `ivfflat` vs `hnsw` (trade-offs lat√™ncia/recall)
  - Dimensionalidade de embeddings: 384 (Sentence-Transformers) vs 768 vs 1536
- [ ] **Estrat√©gias de indexa√ß√£o para escala**
  - Par√¢metros de `ivfflat`: n√∫mero de listas, samples
  - Par√¢metros de `hnsw`: `m` (conectividade) e `ef_construction`
  - Quando re-indexar ap√≥s inser√ß√µes em massa
- [ ] **Postgres Full-Text Search (FTS) para nomes pr√≥prios**
  - Cria√ß√£o de `tsvector` com `to_tsvector('english', text)`
  - Dicion√°rio customizado para nomes de personagens/lugares
  - GIN index em `tsvector` para performance
  - Ranking via `ts_rank` e `ts_rank_cd`
- [ ] **Implementa√ß√£o de Reciprocal Rank Fusion (RRF)**
  - Algoritmo: `score = Œ£ 1/(k + rank_i)` onde `k=60` (padr√£o)
  - Merge de resultados pgvector + FTS em TypeScript
  - Normaliza√ß√£o de scores antes do merge
- [ ] **Chunking sem√¢ntico por se√ß√£o**
  - Parser Markdown: detectar headers (`#`, `##`, `###`)
  - Metadata de contexto: `section_name`, `level`, `parent_section`
  - Tamanho vari√°vel de chunks (100-2000 tokens): handling em embeddings

---

### 3. **Claude 3.5 Sonnet (Anthropic API)**

**Cen√°rios:**
- Extra√ß√£o de entidades/rela√ß√µes de GDD (offline, batch)
- Gera√ß√£o de respostas RAG (runtime, baixa lat√™ncia)
- Prompting h√≠brido (chunks textuais + metadados do grafo)

**T√≥picos para pesquisar:**
- [ ] **Uso eficiente de Claude 3.5 Sonnet no plano Pro**
  - Limites do plano Pro: ~150-200 mensagens/dia (compartilhado CLI + API)
  - Monitoramento de uso via dashboard Anthropic
  - Estrat√©gias de cache de respostas comuns (evitar chamadas redundantes)
  - Quando migrar para pay-per-use ($3 input, $15 output por 1M tokens)
- [ ] **Extra√ß√£o estruturada de entidades com JSON parsing**
  - Prompt engineering para output JSON confi√°vel
  - Handling de respostas malformadas (retry com `max_tokens` maior)
  - Function calling vs prompt engineering: trade-offs
  - Valida√ß√£o de schema (Zod/Joi) p√≥s-extra√ß√£o
- [ ] **Prompt engineering para alta fidelidade em RAG**
  - System prompt: instru√ß√µes de groundedness ("responda APENAS baseado no contexto")
  - Template de contexto: chunks textuais + metadados do grafo (ordem importa?)
  - Few-shot examples: exemplos de boas respostas vs alucina√ß√µes
  - Par√¢metros: `temperature=0.3` (baixa criatividade), `max_tokens=1000` (conciso)
- [ ] **Estrat√©gias de detec√ß√£o de alucina√ß√µes via prompts**
  - T√©cnicas de chain-of-thought: "Liste evid√™ncias do contexto antes de responder"
  - Self-consistency: gerar N respostas, escolher mais consistente
  - Disclaimers expl√≠citos: "Se n√£o souber, diga 'N√£o encontrei no GDD'"
- [ ] **Integra√ß√£o NestJS + Anthropic SDK**
  - `@anthropic-ai/sdk`: configura√ß√£o de API key via `ConfigService`
  - Error handling: rate limits, timeouts, falhas de rede
  - Streaming de respostas (se necess√°rio para UX futura)

---

### 4. **HuggingFace Inference API (Embeddings)**

**Cen√°rios:**
- Gera√ß√£o de embeddings para chunks durante ingest√£o (batch)
- Embedding de queries do usu√°rio (runtime, sub-100ms)
- Free tier: ~30k requests/m√™s

**T√≥picos para pesquisar:**
- [ ] **Modelos de Sentence-Transformers para narrativas**
  - `all-MiniLM-L6-v2`: 384 dim, r√°pido, boa baseline
  - `paraphrase-multilingual-mpnet-base-v2`: 768 dim, suporta portugu√™s
  - `all-mpnet-base-v2`: 768 dim, melhor qualidade para ingl√™s
  - Benchmarks em similaridade narrativa (personagens, eventos)
- [ ] **HuggingFace Inference API: limites e otimiza√ß√µes**
  - Rate limits do free tier: requests/segundo
  - Batch embeddings: enviar m√∫ltiplos textos de uma vez
  - Retry logic para erros 503 (modelo frio)
  - Fallback para OpenAI `text-embedding-3-small` se HF cair
- [ ] **Normaliza√ß√£o e storage de embeddings**
  - Normaliza√ß√£o L2 antes de inserir no pgvector (melhora similaridade)
  - Quantiza√ß√£o de embeddings (reduzir storage): int8 vs float32
  - Compress√£o de √≠ndice: trade-offs de recall vs tamanho
- [ ] **Compara√ß√£o com alternativas**
  - OpenAI `text-embedding-3-small/large`: custo vs qualidade
  - Voyage AI embeddings: especializa√ß√£o em RAG
  - Modelos locais (self-hosted): setup com ONNX/TensorRT

---

### 5. **NestJS (Backend Monol√≠tico)**

**Cen√°rios:**
- Orquestra√ß√£o de pipeline RAG (embedding ‚Üí busca ‚Üí grafo ‚Üí LLM)
- Exposi√ß√£o de APIs REST (`/api/rag/query`, `/api/rag/entities`)
- Isolamento do m√≥dulo `gdd-rag` do resto do Sentinel

**T√≥picos para pesquisar:**
- [ ] **Arquitetura de m√≥dulo isolado em NestJS**
  - Padr√£o de m√≥dulos isolados: `exports: []` (n√£o expor para outros m√≥dulos)
  - Inje√ß√£o de depend√™ncias: `@Injectable()` para services
  - DTOs e valida√ß√£o: `class-validator` para input/output
- [ ] **Integra√ß√£o com PostgreSQL via TypeORM ou Prisma**
  - TypeORM: suporte a queries raw SQL + Cypher (AGE)
  - Prisma: schema para tabelas relacionais, raw SQL para grafos
  - Pool de conex√µes: configura√ß√£o para lat√™ncia m√≠nima
- [ ] **Pipeline sequencial vs paralelo**
  - Sequencial: embedding ‚Üí busca ‚Üí grafo ‚Üí LLM (debugging f√°cil)
  - Paralelo: busca vetorial + FTS em paralelo, merge depois
  - Trade-offs de lat√™ncia vs complexidade
- [ ] **Error handling e logging estruturado**
  - Winston ou Pino: logs JSON para queries/respostas/lat√™ncia
  - Sentry para tracking de erros de LLM (rate limits, timeouts)
  - M√©tricas: Prometheus + Grafana (lat√™ncia P50/P95/P99 por etapa)
- [ ] **Caching de respostas comuns**
  - Redis para cache de queries frequentes (TTL: 1 hora)
  - Cache de embeddings de queries (evitar chamadas HF redundantes)
  - Invalida√ß√£o de cache ao atualizar GDD

---

### 6. **Docker Compose (Infraestrutura Local/VPS)**

**Cen√°rios:**
- Desenvolvimento local reproduz√≠vel
- Deploy em VPS com infra-as-code
- Build de imagem custom (Postgres + AGE + pgvector)

**T√≥picos para pesquisar:**
- [ ] **Dockerfile para Postgres custom (AGE + pgvector)**
  - Base image: `postgres:16`
  - Compila√ß√£o de AGE: depend√™ncias, `make install`
  - Compila√ß√£o de pgvector: `make && make install`
  - Multi-stage build para reduzir tamanho da imagem
- [ ] **docker-compose.yml para stack completa**
  - Services: `postgres`, `nestjs`
  - Volumes persistentes: `postgres_data`
  - Networks: comunica√ß√£o entre containers
  - Health checks: `pg_isready` para garantir Postgres iniciado
- [ ] **Inicializa√ß√£o de schema via `init.sql`**
  - Script executado automaticamente em `/docker-entrypoint-initdb.d/`
  - Ordem de comandos: `CREATE EXTENSION` ‚Üí `create_graph` ‚Üí tabelas ‚Üí √≠ndices
  - Idempot√™ncia: `IF NOT EXISTS` para re-runs
- [ ] **Deploy em VPS (DigitalOcean/Linode)**
  - Setup de Ubuntu 22.04: Docker, Docker Compose, PM2, Nginx
  - CI/CD b√°sico: Git hooks ou GitHub Actions
  - Backup de volumes do Postgres: `pg_dump` + cron
  - SSL via Let's Encrypt (Certbot + Nginx)

---

### 7. **Prompt Engineering & Guardrails (Fase MVP)**

**Cen√°rios:**
- Evitar alucina√ß√µes via system prompts bem estruturados
- Rastreabilidade de respostas (cita√ß√µes de se√ß√µes)
- Valida√ß√£o b√°sica de output (regex, schema)

**T√≥picos para pesquisar:**
- [ ] **System prompts eficazes para groundedness**
  - Estrutura: Papel ‚Üí Regras ‚Üí Formato de Contexto ‚Üí Instru√ß√µes de Resposta
  - Exemplos negativos: "NUNCA invente nomes de personagens..."
  - T√©cnicas de prompt: "Cite a se√ß√£o do GDD de onde extraiu cada informa√ß√£o"
- [ ] **Templates de prompt h√≠brido (chunks + grafo)**
  - Ordem de apresenta√ß√£o: metadados do grafo primeiro vs chunks primeiro?
  - Formata√ß√£o de rela√ß√µes: lista vs grafo textual (ASCII art)
  - Limite de contexto: quantos chunks + quantas entidades/rela√ß√µes?
- [ ] **Valida√ß√µes determin√≠sticas leves**
  - Regex para detectar disclaimers indesejados: `/como ia eu n√£o posso/i`
  - Valida√ß√£o de presen√ßa de nomes de personagens mencionados no contexto
  - Contagem de tokens de resposta (limitar verbose)
- [ ] **Roadmap de guardrails avan√ßados (p√≥s-MVP)**
  - LLM-as-a-judge: Claude Haiku valida groundedness da resposta principal
  - Frameworks: Guardrails AI, NeMo Guardrails (quando adicionar?)
  - M√©tricas de fidelidade: RAGAS (Faithfulness, Context Precision)

---

## üîç √Åreas de Pesquisa por Prioridade

### **Alta Prioridade (Semana 1-2):**
1. Setup de Apache AGE + pgvector no Docker
2. Modelagem de ontologia narrativa em Cypher
3. Integra√ß√£o NestJS + node-postgres para queries Cypher
4. HuggingFace Inference API: autentica√ß√£o, rate limits, batch embeddings
5. Claude 3.5 Sonnet: extra√ß√£o estruturada de entidades (JSON parsing confi√°vel)

### **M√©dia Prioridade (Semana 3-4):**
6. Busca h√≠brida: implementa√ß√£o de RRF em TypeScript
7. Tuning de √≠ndices pgvector (ivfflat vs hnsw)
8. Prompt engineering: templates h√≠bridos (chunks + grafo)
9. Parser Markdown: chunking sem√¢ntico preservando contexto
10. Error handling e logging estruturado no NestJS

### **Baixa Prioridade (P√≥s-MVP):**
11. Compara√ß√£o de modelos de embeddings (HF vs OpenAI vs Voyage)
12. Reranking: Cohere Rerank API, ColBERT v2, cross-encoders
13. Guardrails avan√ßados: LLM-as-a-judge, Guardrails AI
14. M√©tricas de avalia√ß√£o: RAGAS, Faithfulness, Context Precision
15. CI/CD e deploy automatizado (GitHub Actions ‚Üí VPS)

---

## üìñ Estrutura Sugerida para Pr√≥xima Pesquisa

```markdown
# Melhores Pr√°ticas T√©cnicas: Stack RAG para GDDs Narrativos

## 1. PostgreSQL + Apache AGE
### 1.1 Setup e Configura√ß√£o
### 1.2 Modelagem de Ontologia Narrativa
### 1.3 Queries Cypher para Racioc√≠nio Multi-hop
### 1.4 Performance Tuning

## 2. pgvector + Busca H√≠brida
### 2.1 Indexa√ß√£o e Trade-offs (ivfflat vs hnsw)
### 2.2 Postgres Full-Text Search para Keywords
### 2.3 Reciprocal Rank Fusion (RRF)
### 2.4 Chunking Sem√¢ntico

## 3. Claude 3.5 Sonnet (Anthropic)
### 3.1 Extra√ß√£o Estruturada de Entidades
### 3.2 Prompt Engineering para Groundedness
### 3.3 Gest√£o de API do Plano Pro
### 3.4 Integra√ß√£o com NestJS

## 4. HuggingFace Inference API
### 4.1 Modelos Sentence-Transformers
### 4.2 Rate Limits e Otimiza√ß√µes
### 4.3 Compara√ß√£o com Alternativas

## 5. Arquitetura NestJS
### 5.1 M√≥dulo Isolado (gdd-rag)
### 5.2 Pipeline RAG (Sequencial vs Paralelo)
### 5.3 Error Handling e Observability
### 5.4 Caching Estrat√©gico

## 6. Docker + Deploy
### 6.1 Dockerfile Custom (Postgres + AGE + pgvector)
### 6.2 docker-compose.yml
### 6.3 Deploy em VPS
### 6.4 Backup e SSL

## 7. Guardrails e Valida√ß√£o
### 7.1 System Prompts Eficazes
### 7.2 Templates H√≠bridos (Chunks + Grafo)
### 7.3 Valida√ß√µes Determin√≠sticas
### 7.4 Roadmap de Guardrails Avan√ßados
```

---

## ‚úÖ Crit√©rios de Sucesso da Pesquisa

Para cada tecnologia, a pesquisa deve responder:

1. **Setup:** Como configurar? (comandos, depend√™ncias, gotchas)
2. **Best Practices:** Padr√µes recomendados para o cen√°rio narrativo?
3. **Performance:** Tuning para lat√™ncia <2s P95?
4. **Trade-offs:** Quando usar alternativas? (ex: AGE vs Neo4j, HF vs OpenAI)
5. **C√≥digo Exemplo:** Snippets prontos para copiar (TypeScript, SQL, Cypher, Python)
6. **Troubleshooting:** Erros comuns e solu√ß√µes
7. **Custos:** Estimativas (storage, API calls) para 100 designers

---

## üìã Refer√™ncias aos Documentos Base

- **Pesquisa Original:** [Stack RAG Alta Fidelidade para GDDs.md](../docs/pesquisas/Stack%20RAG%20Alta%20Fidelidade%20para%20GDDs.md)
- **Decis√µes Arquiteturais:** [entrevista-stack-rag-gdd-2026-01-29.md](../docs/decisoes-iniciais/entrevista-stack-rag-gdd-2026-01-29.md)

---

**Documento gerado em:** 2026-01-30
**Pr√≥ximo Passo:** Escolher 2-3 t√≥picos de alta prioridade e iniciar pesquisa t√©cnica focada com exemplos de c√≥digo pr√°ticos.
